{\rtf1\ansi\ansicpg1252\cocoartf1347\cocoasubrtf570
{\fonttbl\f0\fswiss\fcharset0 Helvetica;\f1\fnil\fcharset134 STHeitiSC-Light;\f2\fswiss\fcharset0 ArialMT;
\f3\fnil\fcharset0 Menlo-Regular;}
{\colortbl;\red255\green255\blue255;\red28\green28\blue28;}
\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural

\f0\fs24 \cf0 \cb1 1. 
\f1 \'ca\'b2\'c3\'b4\'ca\'c7
\f0 druid\

\f1 \'bc\'dc\'b9\'b9
\f0 \
{\field{\*\fldinst{HYPERLINK "http://www.infoq.com/cn/news/2015/04/druid-data"}}{\fldrslt http://www.infoq.com/cn/news/2015/04/druid-data}}\
\
{\field{\*\fldinst{HYPERLINK "http://blog.csdn.net/derekjiang/article/details/42550991"}}{\fldrslt http://blog.csdn.net/derekjiang/article/details/42550991}}\
\pard\pardeftab720\sl520

\f2\fs28 \cf2 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Druid 
\f1 \'ca\'c7\'d2\'bb\'b8\'f6\'bf\'aa\'d4\'b4\'b5\'c4\'a3\'ac\'c4\'dc\'b9\'bb\'d4\'da\'b4\'f3\'d0\'cd\'ca\'fd\'be\'dd\'bc\'af
\f2  (100\'92s of Billions entries, 100\'92s TB data)
\f1 \'c9\'cf\'c3\'e6\'cc\'e1\'b9\'a9\'ca\'b5\'ca\'b1\'ca\'d4\'cc\'bd\'d0\'d4\'b2\'e9\'d1\'af\'b5\'c4\'b7\'d6\'ce\'f6\'ca\'fd\'be\'dd\'b4\'e6\'b4\'a2\'a3\'ac
\f2 Druid
\f1 \'cc\'e1\'b9\'a9\'c1\'cb\'c1\'ae\'bc\'db\'b5\'c4\'a3\'ac\'b2\'a2\'c7\'d2\'ca\'c7\'b3\'d6\'d0\'f8\'b5\'c4\'ca\'b5\'ca\'b1\'ca\'fd\'be\'dd\'bc\'af\'b3\'c9\'ba\'cd\'c8\'ce\'d2\'e2\'ca\'fd\'be\'dd\'cc\'bd\'cb\'f7\'b5\'c4\'c4\'dc\'c1\'a6\'a1\'a3\
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural

\f3\fs24 \cf0 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 All coming from the druid.io\uc0\u8232 \u8232 \u8232 \u8232 \u8232 OLAP \u8232 	IS: analytics data store \u8232 	WHAT STORE: event data\u8232 	FEA1: real-time \u8232 	FEA2: user-facing analytic \u8232 \u8232 KEY\u8232 	 Column orientation\u8232 	 Inverted indexes\u8232 	 Multi-dimensional\u8232  	 Rolling updates \'97 SaaS should be available all the time - \u8232 	 Trade off between costs and performance \u8232 	 multi-tenants \u8232 	 employs lock-free ingestion \'97\'97 event based\u8232 \u8232 Advantage\u8232 	fast aggregations\u8232 	real-time analysis \u8232 	large data\u8232 	no spot about the data store\u8232 	\u8232 \u8232 High Level\u8232 	gang3 of google search\u8232 	index data to create immutable views, stores in highly opt column format \u8232 	every node to one specific thing\u8232 	Druid\'92s data distribution is segment-based and leverages a highly available "deep" storage such as S3 or HDFS. Scaling up (or down) does not require massive copy actions or downtime; in fact, losing any number of historical nodes does not result in data loss because new historical nodes can always be brought up by reading data from "deep" storage.\u8232 	Druid\'92s data distribution is segment-based\u8232 \u8232 	\u8232 \u8232 \u8232 \u8232 Reference\u8232 	Google Tech > Hadoop ? (dremel(http://www.csdn.net/article/2012-08-21/2808870-Google-Hadoop-versus-Dremel),\u8232 pregel graph-process, caffeine(web indexing), )	\u8232 	PowerDrill \'97  ad hoc(specific queries) - made a column-oriented db\u8232 					         with composite range partition\u8232 						 key data structure\u8232 						 one click trillion valuess\u8232 	Spanner is Google's scalable, multi-version, globally-distributed, and synchronously-replicated database. It is the first system to distribute data at global scale and support externally-consistent distributed transactions.\u8232 	\u8232 \u8232 	With Elasticsearch : Elasticsearch is a search systems based on Apache Lucene\'97\'97High requirements and high costs Druid\u8232 	with Key/Value Stores (HBase/Cassandra/OpenTSDB :  pre-computing the queries -  not suitable for ad-hoc\'97cause processing slow\u8232 dimensions of an event  
\f1 \'a3\'ba
\f3  Commonly used dimensions are people, products, place and time. Timeseries specific databases such as OpenTSDB use this approach\uc0\u8232 only with batch ingestion and are not optimal for streaming data in regularly.\u8232 	With RedShift 
\f1 \'a3\'ba
\f3  only with batch ingestion and are not optimal for streaming data in regularly.Druid\'92s write semantics are not as fluid and does not support full joins (we support large table to small table joins). Redshift provides full SQL support including joins and insert/update statements.\uc0\u8232 ParAccel\'92s data distribution model is hash-based.\u8232 requires re-hashing the data across the nodes, making it difficult to perform without taking downtime. set cluster into read-only mode\u8232 copy data from cluster to new cluster that exists in parallel\u8232 redirect traffic to new cluster. replication strategy. Indexing structures do increase storage overhead \u8232 	With Spark: Complementary spark is suitable for ML(Operations and workflow)\'97\'97druid is focusing on low latency			\u8232 	With Parquet: Parquet is a column storage format that is designed to work with SQL-on-Hadoop engines. Parquet doesn't have a query execution engine, and instead relies on external sources to pull data out of it.Parquet's storage format is much more hierachical, and is more designed for binary chunking.\u8232 		\u8232 \u8232 * Sql on Hadoop\u8232 	Queries\u8232 		queries and results are transferred\u8232 		serde time		\u8232 \u8232 	Data Ingestion\u8232 		Ingest data and query it immediately upon ingestion\u8232 		Plywood\u8232 	Query Flexibility\u8232 \u8232 *Tips\u8232 Metamarkets  OLTP - OLAP - RLAP\u8232 streaming\u8232 \u8232 \u8232 \u8232 \u8232 			\u8232 =============================================================================\u8232 1.OLAP \u8232 \u8232 Simpel event table\u8232 \u8232 Timestamp column: We treat timestamp separately because all of our queries center around the time axis.\u8232 \u8232 Dimension columns: Dimensions are string attributes of an event, and the columns most commonly used in filtering the data. We have four dimensions in our example data set: publisher, advertiser, gender, and country. They each represent an axis of the data that we\'92ve chosen to slice across.\u8232 \u8232 Metric columns: Metrics are columns used in aggregations and computations. In our example, the metrics are clicks and price. Metrics are usually numeric values, and computations include operations such as count, sum, and mean. Also known as measures in standard OLAP terminology.\u8232 \u8232 \u8232 2.Roll up the data\u8232 Druid summarizes this raw data at ingestion time using a process we refer to as "roll-up"\u8232 Roll-up is a first-level aggregation operation over a selected set of dimensions\u8232 minimize the amount of raw data that needs to be stored\u8232 \u8232 Rollup make the data \'97\'97 fail to query simple spec and takes time, \u8232 \u8232 		     \'97  low storage, feature \u8232 		\u8232 		     -	queryGranularity  The lowest supported queryGranularity is millisecond. \u8232 		     -  Druid shards are called segments,along with the indexes for those columns. Druid queries only understand how to scan segments.\u8232 		     -  Segments are uniquely identified by a datasource, interval, version, and an optional partition number.\u8232 		    -Borrowing ideas from search infrastructure, Druid creates immutable snapshots of data, stored in data structures highly optimized for analytic queries.Druid indexes data on a per shard (segment) level.\u8232 \u8232 \u8232 \u8232 \u8232 3.Loading\u8232 		a real-time pipeline for recent insights, and a batch pipeline for the accurate copy of the data.\u8232 \u8232 \u8232 4.Query The Data\u8232 		Druid's native query language is JSON over HTTP\u8232 		Druid is designed to perform single table operations and does not currently support joins. Many production setups do joins at ETL because data must be denormalized before loading into Druid.\u8232 \u8232 \u8232 5. Druid Cluster\u8232 		Each node is designed to do a small set of things very well.\u8232 		Historical nodes download immutable segments locally and serve queries over those segments\u8232 		Broker Nodes Broker nodes are what clients and applications query to get data from Druid. Broker nodes are responsible for scattering queries and gathering and merging results. Broker nodes know what segments live where.\u8232 		Coordinator nodes manage segments on historical nodes in a cluster. Coordinator nodes tell historical nodes to load new segments, drop old segments, and move segments to load balance.\u8232 		Real-time processing in Druid can currently be done using standalone realtime nodes or using the indexing service\u8232 		Real-time processing involves ingesting data, indexing the data (creating segments), and handing segments off to historical nodes. Data is queryable as soon as it is ingested by the realtime processing logic.\u8232 \u8232 \u8232 6. External Dependencies\u8232 		Zookeeper Druid relies on Zookeeper for intra-cluster communication.\u8232 		Metadata Storage \u8232 		MySQL and PostgreSQL are popular metadata stores for production, but Derby can be used for experimentation when you are running all druid nodes on a single machine.\u8232 		The metadata store is not involved in the query path. \u8232 	\u8232 		Deep Storage  Services that create segments upload segments to deep storage and historical nodes download segments from deep storage. Deep storage is not involved in the query path. S3 and HDFS are popular deep storages.\u8232 		Different node types are able to fail without impacting the services of the other node types. To run a highly available Druid cluster, you should have at least 2 nodes of every node type running.\u8232 \u8232 =============================================================================\u8232 . IBM\'92s Netezza[37], HP\'92s Vertica[5], and EMC\'92s Greenplum[29])\u8232   For business intelligence and A-B testing \u8232 \u8232 A shared nothing architecture (SN) is a distributed computing architecture in which each node is independent and self-sufficient, and there is no single point of contention across the system. More specifically, none of the nodes share memory or disk storage.\u8232 =============================================================================      \u8232 *Druid Remain references \u8232        Quick Start - queries  	 \u8232        			\u8232 *	Jmeter     6pm\u8232 \u8232 *       Two DataFrame Comparison\u8232 	\u8232 \u8232 *	spark sql\u8232 \u8232 *	Testing Code	\u8232 \u8232 *	Spark compare druid\u8232 \u8232 	Data frame of spark and druid\u8232 \u8232 \u8232 =============================\u8232 why not  depend on query statistic \u8232 Using the paper as the index to describe other \u8232 the log is not visible to rookie \u8232 converts data stored in the in-memory buffer to a column oriented\u8232 storage format described\u8232 =============================================================================\u8232 Hadoop\'97\'97turn low dataware house into useful resource in business intelligence and A-B testing\u8232 Hadoop works well for storing data, it is not optimized for ingesting\u8232 data and making that data immediately readable\u8232 a great back-office, batch processing, and data warehousing system\u8232 \u8232 an open source, distributed,\u8232 column-oriented, real-time analytical data store. \u8232 \u8232 Real-time nodes maintain an in-memory index buffer for all incoming\u8232 events. These indexes are incrementally populated as events\u8232 are ingested and the indexes are also directly queryable.\u8232 \u8232 a row store for queries on events that exist in this JVM\u8232 heap-based buffer.\u8232 heap overflow problems, real-time\u8232 nodes persist their in-memory indexes to disk either periodically\u8232 or after some maximum row limit is reached.\u8232 \u8232 \u8232 The task\u8232 merges these indexes together and builds an immutable block of\u8232 data that contains all the events that have been ingested by a realtime\u8232 node for some span of time. We refer to this block of data as\u8232 a \'93segment\'94.\u8232 Every\u8232 10 minutes (the persist period is configurable), the node will flush\u8232 and persist its in-memory buffer to disk. Near the end of the hour,\u8232 the node will likely see events for 14:00 to 15:00\u8232 \u8232  Queries will hit both the in-memory and persisted indexes.\u8232 \u8232 \u8232 \u8232 \u8232 Real-time\u8232 nodes ingest data by reading events from the message bus(Kafka)\u8232 , the\u8232 message bus acts as a buffer for incoming events.\u8232 . A message bus\u8232 such as Kafka maintains positional offsets indicating how far a consumer\u8232 (a real-time node) has read in an event stream.recovery and resume the read by the offset in the bus\u8232 as a single endpoint\u8232 from which multiple real-time nodes can read events\u8232 \u8232 \u8232 \u8232 \u8232 Historical node\u8232 encapsulate the functionality to load and serve\u8232 the immutable blocks of data (segments) created by real-time nodes.\u8232 \u8232 \u8232 A shared nothing architecture (SN) is a distributed computing architecture in which each node is independent and self-sufficient, and there is no single point of contention across the system. More specifically, none of the nodes share memory or disk storage.\u8232 \u8232 Costs of Rehashing \'97Ringpop\u8232 \u8232 \u8232 Historical nodes download immutable segments from\u8232 deep storage. Segments must be loaded in memory before they\u8232 can be queried.\u8232 Historical nodes can support read consistency because they only\u8232 deal with immutable data. Immutable data blocks also enable a simple\u8232 parallelization model: historical nodes can concurrently scan\u8232 and aggregate immutable blocks without blocking.\u8232 \u8232 Tiers\u8232 The purpose of\u8232 tiered nodes is to enable higher or lower priority segments to be distributed\u8232 according to their importance\u8232 The \'93hot\'94 cluster and the code one\u8232 \u8232 \u8232 Historical nodes depend on Zookeeper for segment load and unload\u8232 instructions\u8232 Should Zookeeper become unavailable, historical\u8232 nodes are no longer able to serve new data or drop outdated\u8232 data, however, because the queries are served over HTTP, historical\u8232 nodes are still able to respond to query requests for the data they\u8232 are currently serving. This means that Zookeeper outages do not\u8232 impact current data availability on historical nodes.\u8232 \u8232 \u8232 Broker nodes act as query routers to historical and real-time nodes.\u8232 Broker nodes understand the metadata published in Zookeeper about\u8232 what segments are queryable and where those segments are located.\u8232 Broker nodes also merge partial\u8232 results from historical and real-time nodes before returning a final\u8232 consolidated result to the caller.\u8232 published in Zookeeper about\u8232 what segments are queryable \u8232 \u8232 Broker - caching\u8232 	using local heap or the memcached db/K/V\u8232 \u8232 Real-time\u8232 data is never cached and hence requests for real-time data will always\u8232 be forwarded to real-time nodes. Real-time data is perpetually\u8232 changing and caching the results is unreliable.\u8232 If broker nodes are unable to communicate to Zookeeper, they use\u8232 their last known view of the cluster and continue to forward queries\u8232 to real-time and historical nodes. \u8232 \u8232 \u8232 Druid coordinator nodes are primarily in charge of data management\u8232 and distribution on historical nodes\u8232 \u8232 multi-version\u8232 concurrency control swapping protocol for managing immutable\u8232 segments\u8232  The remaining coordinator nodes\u8232 act as redundant backups.\u8232 \u8232 Coordinator\u8232 nodes also maintain a connection to a MySQL database that contains\u8232 additional operational parameters and configurations. One of\u8232 the key pieces of information located in the MySQL database is a\u8232 table that contains a list of all segments that should be served by\u8232 historical nodes. This table can be updated by any service that creates\u8232 segments, for example, real-time nodes. The MySQL database\u8232 also contains a rule table that governs how segments are created,\u8232 destroyed, and replicated in the cluster.s\u8232 \u8232 : if an external dependency responsible for coordination\u8232 fails, the cluster maintains the status quo. \u8232 \u8232 \u8232 
\f1 \'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd
\f3 \uc0\u8232 Data tables in Druid (called data sources) are collections of timestamped\u8232 events and partitioned into a set of segments, where each\u8232 segment is typically 5\'9610 million rows. Formally, we define a segment\u8232 as a collection of rows of data that span some period of time.\u8232 Segments represent the fundamental storage unit in Druid and replication\u8232 and distribution are done at a segment level.\u8232 \u8232 Broker process the query\u8232 \u8232 a timestamp column partition\u8232 The time granularity to partition segments is a function\u8232 of data volume and time range.\u8232 \u8232 The version string indicates the\u8232 freshness of segment data;\u8232 \u8232 Druid segments are stored in a column orientation. \u8232 \u8232 different compression methods\u8232 are used to reduce the cost of storing a column in memory and\u8232 on disk.\u8232 \u8232 Storing strings directly is\u8232 unnecessarily costly and string columns can be dictionary encoded\u8232 instead. \u8232 string columns can be dictionary encoded\u8232 instead.\u8232 \u8232 \u8232 Druid uses the LZF [24] compression\u8232 algorithm\u8232 \u8232 \u8232 Druid creates additional lookup indices for\u8232 string columns such that only those rows that pertain to a particular\u8232 query filter are ever scanned.\u8232 \u8232 \u8232 
\f1 \'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd
\f3 \uc0\u8232 Druid creates additional lookup indices for\u8232 string columns such that only those rows that pertain to a particular\u8232 query filter are ever scanned.\u8232 \u8232 Justin Bieber -> rows [0, 1] -> [1][1][0][0]\u8232 Ke$ha -> rows [2, 3] -> [0][0][1][1]\u8232 Justin Bieber is seen in rows 0 and 1. This mapping of column\u8232 values to row indices forms an inverted index [39]. To know\u8232 which rows contain Justin Bieber or Ke$ha, we can OR together\u8232 the two arrays.\u8232 [0][1][0][1] OR [1][0][1][0] = [1][1][1][1]\u8232 \u8232 What is interesting to note is that after sorting, global compression\u8232 only increased minimally\u8232 \u8232 \u8232 
\f1 \'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd\'a3\'bd
\f3 \uc0\u8232 Druid has its own query language and accepts queries as POST\u8232 requests. Broker, historical, and real-time nodes all share the same\u8232 query API.\u8232 \u8232 \u8232  A typical query\u8232 will contain the data source name, the granularity of the result data,\u8232 time range of interest, the type of request, and the metrics to aggregate\u8232 over. The result will also be a JSON object containing the\u8232 aggregated metrics over the time period.\u8232 \u8232 \u8232 Most query types will also support a filter set. A filter set is a\u8232 Boolean expression of dimension name and value pairs.\u8232 \u8232 \u8232 \{\u8232 "queryType" : "timeseries",\u8232 "dataSource" : "wikipedia",\u8232 "intervals" : "2013-01-01/2013-01-08",\u8232 "filter" : \{\u8232 "type" : "selector",\u8232 "dimension" : "page",\u8232 "value" : "Ke$ha"\u8232 \},\u8232 "granularity" : "day",\u8232 "aggregations" : [\{"type":"count", "name":"rows"\}]\u8232 \}\u8232 \u8232 As of this writing, a join query for Druid is not yet implemented.\u8232 A join query is essentially the merging of two or more streams of\u8232 data based on a shared set of keys. \u8232 \u8232 The primary high-level strategies\u8232 for join queries we are aware of are a hash-based strategy or a\u8232 sorted-merge strategy. \u8232 \u8232 We also present Druid benchmarks on TPC-H data. Most TPC-H\u8232 queries do not directly apply to Druid, so we selected queries more\u8232 typical of Druid\'92s workload to demonstrate query performance. As\u8232 a comparison, we also provide the results of the same queries using\u8232 MySQL using the MyISAM engine (InnoDB was slower in our\u8232 experiments).\u8232 \u8232 \u8232 In this paper we presented Druid, a distributed, column-oriented,\u8232 real-time analytical data store. Druid is designed to power high\u8232 performance applications and is optimized for low query latencies.\u8232 Druid supports streaming data ingestion and is fault-tolerant. \u8232 =============================================================================\u8232 Configuration: \u8232 \u8232 \u8232 \u8232 	\
}